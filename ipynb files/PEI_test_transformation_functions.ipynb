{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8f8565-83b2-47a3-930f-860426316cc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install nutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f1f0d8-cad5-4b8a-9894-aa2ab19b9447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/vishnuas1987@gmail.com/PEI_Case_Study/Functions/PEI_transformation_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90814dcb-9ff4-4a5e-86a8-9f05cbc286d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType,BooleanType,LongType,DoubleType,FloatType\n",
    "import runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7050a156-de08-4af5-b7d1-50882bc04d51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNotebook: N/A - Lifecycle State: N/A, Result: N/A\nRun Page URL: N/A\n============================================================\nPASSING TESTS\n------------------------------------------------------------\naggregate_dataframe (1.088064380994183 seconds)\nclean_phone_number (0.8460635490046116 seconds)\nclean_product_name (0.7304145429952769 seconds)\nenrich_name (0.7773054360004608 seconds)\nremove_non_alphabet (0.8455801029922441 seconds)\n\n\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "from runtime.nutterfixture import NutterFixture,tag\n",
    "class PEItestFixture(NutterFixture):\n",
    "    #Test the function remove_non_alphabet\n",
    "    def assertion_remove_non_alphabet(self):\n",
    "        data = [(1,'Ad.       ..am Hart'),(2,'Tam&^*ara Willing___)ham'),(3,'Pete@#$ Takahito'),(4, 'johnXahiur')]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df = spark.createDataFrame(data,schema)\n",
    "        data = [(1,'Adam Hart'),(2,'Tamara Willingham'),(3,'Pete Takahito'),(4, 'John Xahiur')]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df_expected = spark.createDataFrame(data,schema)\n",
    "        df_actual = remove_non_alphabet(df,'Name')\n",
    "        assert df_actual.collect() == df_expected.collect() \n",
    "\n",
    "    #test the function clean_phone_number\n",
    "    def assertion_clean_phone_number(self):\n",
    "        data = [(1,'001-581-945-5641'),\n",
    "                (2,'(128)935-6357x6738'),\n",
    "                (3,'713.315.6216x96588'),\n",
    "                (4, '202-494-3717'),\n",
    "                (5, '+1-556-9469745x3698'),\n",
    "                (6, '563444'),\n",
    "                (7, 'x563444'),\n",
    "                (8, '-5566'),\n",
    "                ]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df = spark.createDataFrame(data,schema)\n",
    "        data = [(1,'+1(581)945-5641'),\n",
    "                (2,'+1(128)935-6357x6738'),\n",
    "                (3,'+1(713)315-6216x96588'),\n",
    "                (4, '+1(202)494-3717'),\n",
    "                (5,'+1(556)946-9745x3698'),\n",
    "                (6, 'Invalid phone number'),\n",
    "                (7, 'Invalid phone number'),\n",
    "                (8, 'Invalid phone number')\n",
    "                ]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df_expected = spark.createDataFrame(data,schema)\n",
    "        df_actual = clean_phone_number(df,'Name')\n",
    "        assert df_actual[\"Id\",\"_cleaned_Name\"].collect() == df_expected.collect()  \n",
    "\n",
    "    #test the function to clean the product name\n",
    "    def assertion_clean_product_name(self):\n",
    "        data = [\n",
    "             (1,'Tenex Chairmat w/ Average Lip, 45\" x 53\"'),\\\n",
    "             (2,'ImationÂ USB 2.0 SwivelÂ Flash DriveÂ USBÂ flash driveÂ - 4 GB - Pink'),\\\n",
    "             (3,'\"While you Were Out\" Message Book, One Form per Page'),\\\n",
    "             (4, 'ImationÂ 30456 USBÂ Flash DriveÂ 8GB'),\\\n",
    "             (5, 'Redi-Strip #10 Envelopes, 4 1/8 x 9 1/2')\n",
    "             ]   \n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df = spark.createDataFrame(data,schema)\n",
    "        data = [\n",
    "            (1,'Tenex Chairmat w/ Average Lip, 45\" x 53\"'),\n",
    "            (2,'Imation USB 2.0 Swivel Flash Drive USB flash drive - 4 GB - Pink'),\n",
    "            (3,'\"While you Were Out\" Message Book, One Form per Page'),\n",
    "            (4, 'Imation 30456 USB Flash Drive 8GB'),\n",
    "            (5, 'Redi-Strip #10 Envelopes, 4 1/8 x 9 1/2')\n",
    "            ]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df_expected = spark.createDataFrame(data,schema)      \n",
    "        df_actual = clean_product_name(df,'Name')\n",
    "        assert df_actual.collect() == df_expected.collect() \n",
    "\n",
    "    #test the function to enrich the name field\n",
    "    def assertion_enrich_name(self):\n",
    "        data = [\n",
    "             (1,'Lisa Ryan',''),\\\n",
    "             (2,'','clarencehughes280@gmail.com'),\\\n",
    "             (3,'','bradleywilliams694'),\\\n",
    "             (4, '',''),\\\n",
    "             (5, '','@gmail.com')\n",
    "             ]   \n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True),\\\n",
    "                StructField(\"Email\", StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df = spark.createDataFrame(data,schema)        \n",
    "        data = [\n",
    "            (1,'Lisa Ryan','',False),\n",
    "            (2,'clarencehughes','clarencehughes280@gmail.com',True),\n",
    "            (3,'bradleywilliams','bradleywilliams694',True),\n",
    "            (4, '','',True),\n",
    "            (5, '','@gmail.com',True)\n",
    "            ]\n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Name\", StringType(), True),\\\n",
    "                StructField(\"Email\", StringType(), True),\\\n",
    "                StructField(\"name_filled\", BooleanType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df_expected = spark.createDataFrame(data,schema)    \n",
    "        df_actual = enrich_name(df,'Name','Email')\n",
    "        assert df_actual.collect() == df_expected.collect() \n",
    "\n",
    "    #test the function to aggregrate the dataframe\n",
    "    def assertion_aggregate_dataframe(self):\n",
    "        data = [\n",
    "            (1,'2014','AB-10105','Office Supplies','Binders',103.992),\n",
    "            (2,'2014','PF-19165','Furniture','Tables',124.4615385),\n",
    "            (3,'2015','KB-16240','Furniture','Tables',124.4615385),\n",
    "            (4, '2015','PF-19165','Furniture','Chairs',284.6666667),\n",
    "            (5, '2015','PF-19165','Office Supplies','Paper',60.4),\n",
    "            (6, '2015','PF-19165','Furniture','Chairs',284.6666667),\n",
    "            ]  \n",
    "        schema = StructType(\n",
    "            [\n",
    "                StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Order_year\", StringType(), True), \\\n",
    "                StructField(\"Customer_ID\", StringType(), True),\\\n",
    "                StructField(\"Category\", StringType(), True),\\\n",
    "                StructField(\"Sub_Category\", StringType(), True),\\\n",
    "                StructField(\"Profit\", DoubleType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df = spark.createDataFrame(data,schema)        \n",
    "        data = [\n",
    "            ('2014','AB-10105','Office Supplies','Binders',103.992),\n",
    "            ('2014','PF-19165','Furniture','Tables',124.4615385),\n",
    "            ('2015','KB-16240','Furniture','Tables',124.4615385),\n",
    "            ('2015','PF-19165','Furniture','Chairs',569.3333334),\n",
    "            ('2015','PF-19165','Office Supplies','Paper',60.4)\n",
    "            ] \n",
    "        schema = StructType(\n",
    "            [\n",
    "                # StructField(\"Id\", IntegerType(), True), \\\n",
    "                StructField(\"Order_year\", StringType(), True), \\\n",
    "                StructField(\"Customer_ID\", StringType(), True),\\\n",
    "                StructField(\"Category\", StringType(), True),\\\n",
    "                StructField(\"Sub_Category\", StringType(), True),\\\n",
    "                StructField(\"Total_Profit\", DoubleType(), True)\n",
    "            ]\n",
    "        )\n",
    "        df_expected = spark.createDataFrame(data,schema)   \n",
    "        df_actual = aggregate_dataframe(df,\"Order_year\",\"Customer_ID\",\"Category\",\"Sub_Category\",\"Profit\")\n",
    "        assert df_actual.collect() == df_expected.collect() \n",
    "\n",
    "result = PEItestFixture().execute_tests()\n",
    "print(result.to_string())     "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PEI_test_transformation_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
